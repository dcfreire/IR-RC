{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.8.1 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jsonlines\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init(mem=10000)\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "%env TERRIER_HEAP_MEM=10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTerrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(\"data/corpus.jsonl\", \"r\") as fp:\n",
    "    fp = [\n",
    "        {\n",
    "            \"docno\": line[\"id\"],\n",
    "            \"kw\": \" \".join(line[\"keywords\"]) + \" \" + line[\"title\"],\n",
    "            \"body\": line[\"title\"] + \" \" + line[\"text\"] + \" \" + \" \".join(line[\"keywords\"]),\n",
    "        }\n",
    "        | line\n",
    "        for line in fp\n",
    "    ]\n",
    "    iter_indexer = pt.IterDictIndexer(\n",
    "        \"./index_body2\",\n",
    "        fields=[\"text\", \"title\", \"keywords\", \"kw\", \"body\"],\n",
    "        meta=[\"docno\", \"text\", \"title\", \"keywords\", \"kw\", \"body\"],\n",
    "        meta_lengths=[20, 10000, 10000, 10000, 10000],\n",
    "        blocks=True,\n",
    "    )\n",
    "    iter_indexer.index(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexref = pt.IndexRef.of(\"./index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyserini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with jsonlines.open(\"data/corpus.jsonl\", \"r\") as fp:\n",
    "    out = []\n",
    "    for line in fp:\n",
    "        out.append({\"id\": line[\"id\"], \"contents\": line[\"title\"] + \"\\n\" + line[\"text\"] + \"\\n\" + \" \".join(line[\"keywords\"])})\n",
    "\n",
    "    with open(f\"c/corpus.json\", \"w\") as f:\n",
    "        json.dump(out, f)\n",
    "\n",
    "!python -m pyserini.index.lucene --collection JsonCollection --input c --index indexes/sparse --generator DefaultLuceneDocumentGenerator --threads 1 --storePositions --storeDocvectors --storeRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pyserini.search.lucene --index indexes/sparse/ --topics test_queries.tsv --output runs/run.train.bm25.trec --output-format trec --hits 1000 --bm25 --k1 0.82 --b 0.68\n",
    "!python process_results.py run/run.train.bm25.trec > out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LambdaMART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = pd.read_csv(\"data/train_queries.csv\", dtype={\"QueryId\": str, \"Query\": str}).rename(columns={\"QueryId\": \"qid\", \"Query\": \"query\"})\n",
    "q_test = pd.read_csv(\"data/test_queries.csv\", dtype={\"QueryId\": str, \"Query\": str}).rename(columns={\"QueryId\": \"qid\", \"Query\": \"query\"})\n",
    "qrel = pd.read_csv(\"data/train_qrels.csv\", dtype={\"QueryId\": str, \"EntityId\": str, \"Relevance\": int}).rename(columns={\"QueryId\": \"qid\", \"EntityId\": \"docno\", \"Relevance\": \"label\"})\n",
    "q[\"query\"] = q[\"query\"].str.replace(\"'s\", \"\")\n",
    "q[\"query\"] = q[\"query\"].str.replace(\"'\", \"\")\n",
    "q[\"query\"] = q[\"query\"].str.replace(r\"[^\\w\\s]\", \"\")\n",
    "q_test[\"query\"] = q_test[\"query\"].str.replace(\"'s\", \"\")\n",
    "q_test[\"query\"] = q_test[\"query\"].str.replace(\"'\", \"\")\n",
    "q_test[\"query\"] = q_test[\"query\"].str.replace(r\"[^\\w\\s]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(indexref, wmodel=\"BM25\", controls={\"c\" : 0.4, \"bm25.k_1\": 0.9, \"bm25.k_3\": 0.5})\n",
    "tf = pt.BatchRetrieve(indexref, wmodel=\"Tf\")\n",
    "pl2 = pt.BatchRetrieve(indexref, wmodel=\"PL2\")\n",
    "cm = pt.BatchRetrieve(indexref, wmodel=\"CoordinateMatch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.39578\n",
    "\n",
    "sdm = pt.rewrite.SDM()\n",
    "rm3_pipe = bm25 >> pt.rewrite.RM3(indexref) >> bm25\n",
    "\n",
    "pipeline = (\n",
    "    bm25\n",
    "    >> pt.text.get_text(indexref, [\"title\", \"text\", \"kw\"])\n",
    "    >> (\n",
    "        pt.transformer.IdentityTransformer()\n",
    "        ** (sdm >> bm25)\n",
    "        ** cm\n",
    "        ** (pt.text.scorer(body_attr=\"title\", wmodel=\"BM25\", background_index=indexref))\n",
    "        ** (pt.text.scorer(body_attr=\"kw\", wmodel=\"BM25\", background_index=indexref))\n",
    "        ** (pt.text.scorer(body_attr=\"title\", wmodel=\"CoordinateMatch\", background_index=indexref))\n",
    "    )\n",
    ")\n",
    "lmart = lgb.LGBMRanker(\n",
    "    task=\"train\",\n",
    "    silent=False,\n",
    "    min_data_in_leaf=1,\n",
    "    min_sum_hessian_in_leaf=1,\n",
    "    max_bin=255,\n",
    "    num_leaves=31,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    ndcg_eval_at=[100],\n",
    "    ndcg_at=[100],\n",
    "    eval_at=[100],\n",
    "    learning_rate=0.1,\n",
    "    importance_type=\"gain\",\n",
    "    num_iterations=100,\n",
    "    early_stopping_rounds=5,\n",
    "    n_jobs=16\n",
    ")\n",
    "train, val = train_test_split(q, test_size=0.2)\n",
    "lmart_pipe = pipeline >> pt.ltr.apply_learned_model(lmart, form=\"ltr\")\n",
    "lmart_pipe.fit(train, qrel, val, qrel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = pd.DataFrame(columns=['qid', 'docno', 'label'])\n",
    "pt.Experiment([lmart_pipe % 100], q_test, empty, [\"ndcg_cut_100\"], names=[\"LTR\"], save_dir=\"results/\", filter_by_topics=False, filter_by_qrels=False, save_mode=\"overwrite\")\n",
    "!python process_results.py results/LTR.res.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier_bert.pyt_cedr import CEDRPipeline\n",
    "\n",
    "\n",
    "dph = pt.BatchRetrieve(indexref, controls={\"wmodel\" : \"DPH\"}, verbose=True, metadata=[\"docno\", \"body\"])\n",
    "cedrpipe = dph >> CEDRPipeline(max_valid_rank=20)\n",
    "train, val = train_test_split(q, test_size=0.2)\n",
    "\n",
    "cedrpipe.fit(train, qrel, val, qrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = pd.DataFrame(columns=['qid', 'docno', 'label'])\n",
    "pt.Experiment([cedrpipe % 100], q_test, empty, [\"map\", \"ndcg_cut_100\"], names=[\"BERT\"], save_dir=\"results/\", filter_by_topics=False, filter_by_qrels=False, save_mode=\"overwrite\")\n",
    "\n",
    "!python process_results.py results/BERT.res.gz > out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained rankers\n",
    "\n",
    "All of pyserini reproduction guides were tested (with the providaded already tuned models)\n",
    "https://github.com/castorini/pyserini (Did not include them here because they all basically consist of one very similar line, usually only changing the encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import onir_pt\n",
    "\n",
    "indexed_epic = onir_pt.indexed_epic.from_checkpoint('https://macavaney.us/epic.msmarco.tar.gz', index_path='./epic_cord19')\n",
    "with jsonlines.open(\"data/corpus.jsonl\", \"r\") as fp:\n",
    "    fp = [{\"docno\": line[\"id\"]} | line for line in fp]\n",
    "    indexed_epic.index(fp, fields=('title', 'text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT + LTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = pd.read_csv(\"data/train_queries.csv\", dtype={\"QueryId\": str, \"Query\": str}).rename(columns={\"QueryId\": \"qid\", \"Query\": \"query\"})\n",
    "q_test = pd.read_csv(\"data/test_queries.csv\", dtype={\"QueryId\": str, \"Query\": str}).rename(columns={\"QueryId\": \"qid\", \"Query\": \"query\"})\n",
    "qrel = pd.read_csv(\"data/train_qrels.csv\", dtype={\"QueryId\": str, \"EntityId\": str, \"Relevance\": int}).rename(columns={\"QueryId\": \"qid\", \"EntityId\": \"docno\", \"Relevance\": \"label\"})\n",
    "q[\"query\"] = q[\"query\"].str.replace(\"'s\", \"\")\n",
    "q[\"query\"] = q[\"query\"].str.replace(\"'\", \"\")\n",
    "q[\"query\"] = q[\"query\"].str.replace(r\"[^\\w\\s]\", \"\")\n",
    "q_test[\"query\"] = q_test[\"query\"].str.replace(\"'s\", \"\")\n",
    "q_test[\"query\"] = q_test[\"query\"].str.replace(\"'\", \"\")\n",
    "q_test[\"query\"] = q_test[\"query\"].str.replace(r\"[^\\w\\s]\", \"\")\n",
    "empty = pd.DataFrame(columns=['qid', 'docno', 'label'])\n",
    "\n",
    "qhr = qrel[qrel[\"label\"] > 1] #  high relevance\n",
    "qlr = qrel[qrel[\"label\"] == 1] # low relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT HIGH REL\n",
    "\n",
    "DPH_br = pt.BatchRetrieve(indexref, controls={\"wmodel\" : \"DPH\"}, verbose=True, metadata=[\"docno\", \"body\"])\n",
    "cedrpipehr = DPH_br >> CEDRPipeline(max_valid_rank=20)\n",
    "train, val = train_test_split(q, test_size=0.2)\n",
    "\n",
    "cedrpipehr.fit(train, qhr, val, qhr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT LOW REL\n",
    "\n",
    "DPH_br = pt.BatchRetrieve(indexref, controls={\"wmodel\" : \"DPH\"}, verbose=True, metadata=[\"docno\", \"body\"])\n",
    "cedrpipelr = DPH_br >> CEDRPipeline(max_valid_rank=20)\n",
    "\n",
    "cedrpipelr.fit(train, qlr, val, qlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT KEYWORDS\n",
    "\n",
    "DPH_br = pt.BatchRetrieve(indexref, controls={\"wmodel\" : \"DPH\"}, verbose=True, metadata=[\"docno\", \"body\", \"kw\"])\n",
    "cedrpipekw = DPH_br >>  CEDRPipeline(max_valid_rank=20, doc_attr=\"kw\")\n",
    "train, val = train_test_split(q, test_size=0.2)\n",
    "\n",
    "cedrpipekw.fit(train, qlr, val, qlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT TITLES\n",
    "\n",
    "DPH_br = pt.BatchRetrieve(indexref, controls={\"wmodel\" : \"DPH\"}, verbose=True, metadata=[\"docno\", \"body\", \"title\"])\n",
    "cedrpipetitle = DPH_br >> CEDRPipeline(max_valid_rank=20, doc_attr=\"title\")\n",
    "\n",
    "cedrpipetitle.fit(train, qlr, val, qlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT TEXT\n",
    "\n",
    "DPH_br = pt.BatchRetrieve(indexref, controls={\"wmodel\" : \"DPH\"}, verbose=True, metadata=[\"docno\", \"body\", \"text\"])\n",
    "cedrpipetext = DPH_br >> CEDRPipeline(max_valid_rank=20, doc_attr=\"text\")\n",
    "\n",
    "cedrpipetext.fit(train, qlr, val, qlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (\n",
    "    DPH_br\n",
    "    >> (\n",
    "        cedrpipetext\n",
    "        ** cedrpipekw\n",
    "        ** cedrpipetitle\n",
    "    )\n",
    ")\n",
    "\n",
    "lmart = lgb.LGBMRanker(\n",
    "    task=\"train\",\n",
    "    silent=False,\n",
    "    min_data_in_leaf=1,\n",
    "    min_sum_hessian_in_leaf=1,\n",
    "    max_bin=255,\n",
    "    num_leaves=31,\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    ndcg_eval_at=[100],\n",
    "    ndcg_at=[100],\n",
    "    eval_at=[100],\n",
    "    learning_rate=0.1,\n",
    "    importance_type=\"gain\",\n",
    "    num_iterations=100,\n",
    "    early_stopping_rounds=5,\n",
    "    n_jobs=16\n",
    ")\n",
    "train, val = train_test_split(q, test_size=0.2)\n",
    "lmart_pipe = pipeline >> pt.ltr.apply_learned_model(lmart, form=\"ltr\")\n",
    "lmart_pipe.fit(train, qrel, val, qrel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = pd.DataFrame(columns=['qid', 'docno', 'label'])\n",
    "\n",
    "pt.Experiment([lmart_pipe % 100], q_test, empty, [\"ndcg_cut_100\"], names=[\"LTR\"], save_dir=\"results/\", filter_by_topics=False, filter_by_qrels=False, save_mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastrank\n",
    "bm25 = pt.BatchRetrieve(indexref, wmodel=\"BM25\", controls={\"c\" : 0.4, \"bm25.k_1\": 0.9, \"bm25.k_3\": 0.5})\n",
    "\n",
    "ltr = bm25 >> (cedrpipehr ** cedrpipelr)\n",
    "train_request = fastrank.TrainRequest.coordinate_ascent()\n",
    "params = train_request.params\n",
    "params.init_random = True\n",
    "params.normalize = True\n",
    "ca_pipe = ltr >> pt.ltr.apply_learned_model(train_request, form='fastrank')\n",
    "\n",
    "\n",
    "ca_pipe.fit(q, qrel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = pd.DataFrame(columns=['qid', 'docno', 'label'])\n",
    "\n",
    "pt.Experiment([ca_pipe % 100], q_test, empty, [\"ndcg_cut_100\"], names=[\"LTR\"], save_dir=\"results/\", filter_by_topics=False, filter_by_qrels=False, save_mode=\"overwrite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
